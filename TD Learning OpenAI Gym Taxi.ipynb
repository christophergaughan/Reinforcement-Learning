{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### The Taxi Problem\n",
    "\n",
    "There are 4 designated locations in the grid world indicated by `R(ed)`, `G(reen)`, `Y(ellow)`, and `B(lue)` and the task is to pick up the passenger at one location and drop him off in another. The agent receives `+20` points for a successful dropoff, and loses 1 point for every timestep it takes. There is also a `10` point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "    \n",
    "##### Observations\n",
    "There are `500` discrete states since there are `25` taxi positions, `5` possible locations of the passenger (including the case when the passenger is in the taxi), and `4` destination locations.\n",
    "State encoding: `(taxi_row, taxi_col, passenger_location, destination)`\n",
    "###### Passenger locations\n",
    " - 0: R(ed)\n",
    " - 1: G(reen)\n",
    " - 2: Y(ellow)\n",
    " - 3: B(lue)\n",
    " - 4: in taxi\n",
    "###### Destinations\n",
    " - 0: R(ed)\n",
    " - 1: G(reen)\n",
    " - 2: Y(ellow)\n",
    " - 3: B(lue)\n",
    "\n",
    "#### Actions\n",
    "There are 6 discrete deterministic actions:\n",
    " - 0: move south\n",
    " - 1: move north\n",
    " - 2: move east\n",
    " - 3: move west\n",
    " - 4: pickup passenger\n",
    " - 5: drop off passenger\n",
    "\n",
    "#### Rewards\n",
    "There is a default per-step reward of `-1`, except for delivering the passenger, which is `+20`, or executing `pickup` and `drop-off` actions illegally, which is `-10`.\n",
    "\n",
    "#### Rendering\n",
    " - blue: passenger\n",
    " - magenta: destination\n",
    " - yellow: empty taxi\n",
    " - green: full taxi\n",
    " - letters (R, G, Y and B): possible locations for passengers destinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def interact(env, agent, num_episodes=20000, window=100):\n",
    "    '''\n",
    "    Monitor agent's performance.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "    - env: instance of OpenAI Gym's Taxi-v1 environment\n",
    "    - agent: instance of class Agent (see Agent.py for details)\n",
    "    - num_episodes: number of episodes of agent-environment interaction\n",
    "    - window: number of episodes to consider when calculating average rewards\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    - avg_rewards: deque containing average rewards\n",
    "    - best_avg_reward: largest value in the avg_rewards deque\n",
    "    '''\n",
    "    # initialize average rewards\n",
    "    avg_rewards = deque(maxlen=num_episodes)\n",
    "    # initialize best average reward\n",
    "    best_avg_reward = -math.inf\n",
    "    # initialize monitor for most recent rewards\n",
    "    samp_rewards = deque(maxlen=window)\n",
    "    # for each episode\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # begin the episode\n",
    "        state = env.reset()\n",
    "        # initialize the sampled reward\n",
    "        samp_reward = 0\n",
    "        while True:\n",
    "            # agent selects an action\n",
    "            action = agent.select_action(state)\n",
    "            # agent performs the selected action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # agent performs internal updates based on sampled experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            # update the sampled reward\n",
    "            samp_reward += reward\n",
    "            # update the state (s <- s') to next time step\n",
    "            state = next_state\n",
    "            if done:\n",
    "                # save final sampled reward\n",
    "                samp_rewards.append(samp_reward)\n",
    "                break\n",
    "        if (i_episode >= 100):\n",
    "            # get average reward from last 100 episodes\n",
    "            avg_reward = np.mean(samp_rewards)\n",
    "            # append to deque\n",
    "            avg_rewards.append(avg_reward)\n",
    "            # update best average reward\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "        # monitor progress\n",
    "        print(\"\\rEpisode {}/{} || Best average reward {}\".format(i_episode, num_episodes, best_avg_reward), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        # check if task is solved (according to OpenAI Gym)\n",
    "        if best_avg_reward >= 9.7:\n",
    "            print('\\nEnvironment solved in {} episodes.'.format(i_episode), end=\"\")\n",
    "            break\n",
    "        if i_episode == num_episodes:\n",
    "            print('\\n')\n",
    "        time.sleep(0.001) # 1ms\n",
    "    return avg_rewards, best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, nA=6):\n",
    "        \"\"\" Initialize agent.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - nA: number of actions available to the agent\n",
    "        \"\"\"\n",
    "        self.nA = nA\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
    "        self.lr = 0.2\n",
    "        self.gamma = 1.0\n",
    "        self.epsilon = 0.0005\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\" Given the state, select an action.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - state: the current state of the environment\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "        - action: an integer, compatible with the task's action space\n",
    "        \"\"\"\n",
    "        policy = [self.epsilon / self.nA] * self.nA\n",
    "        policy[np.argmax(self.Q[state])] += 1 - self.epsilon\n",
    "        return np.random.choice(np.arange(self.nA), p=policy)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Update the agent's knowledge, using the most recently sampled tuple.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - state: the previous state of the environment\n",
    "        - action: the agent's previous choice of action\n",
    "        - reward: last reward received\n",
    "        - next_state: the current state of the environment\n",
    "        - done: whether the episode is complete (True or False)\n",
    "        \"\"\"\n",
    "        if done:\n",
    "            self.Q[state][action] += self.lr * (20 - self.Q[state][action])\n",
    "            return\n",
    "        \n",
    "        # SARSA\n",
    "        # self.Q[state][action] += self.lr * (reward + self.gamma * self.Q[next_state][self.select_action(next_state)] - self.Q[state][action])\n",
    "        \n",
    "        # Expected-SARSA\n",
    "        # policy = [self.epsilon / self.nA] * self.nA\n",
    "        # policy[np.argmax(self.Q[next_state])] += 1 - self.epsilon\n",
    "        # self.Q[state][action] += self.lr * (reward + self.gamma * sum([policy[i] * self.Q[next_state][a] for i, a in enumerate(range(self.nA))]) - self.Q[state][action])\n",
    "        \n",
    "        # Q Learning\n",
    "        self.Q[state][action] += self.lr * (reward + self.gamma * max([self.Q[next_state][a] for a in range(self.nA)]) - self.Q[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "agent = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "avg_rewards, best_avg_reward = interact(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ],
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "clear_output(wait=True)\n",
    "print(env.render(mode='ansi'))\n",
    "while True:\n",
    "    action = agent.select_action(state)\n",
    "    state, _, done, _ = env.step(action)\n",
    "    time.sleep(1)\n",
    "    clear_output(wait=True)\n",
    "    print(env.render(mode='ansi'))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "Next: [RL in Continuous Spaces](./RL%20Continuous%20Spaces.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

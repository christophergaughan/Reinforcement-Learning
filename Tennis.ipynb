{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Multi Player Tennis\n",
    "\n",
    "In this `Tennis` environment, two agents control rackets to play the game of tennis in accordance with the rules of the game.\n",
    "\n",
    "An agent receives `+ve` rewards for the following 2 cases:\n",
    "- `+0.10` for hitting the ball over the net while landing it within the bounds of the opponent's court\n",
    "- `+0.50` for refraining itself from a volley, when the ball eventually lands outside the boundary of it own court\n",
    "\n",
    "Otherwise, it receives `-ve` rewards for all the following scenarios:\n",
    "- `-0.50` for missing a valid shot from the opponent\n",
    "- `-0.05` for hitting the ball (without landing it) out of bounds of its opponent's court\n",
    "\n",
    "It receives `-0.01` for all other cases:\n",
    "- hitting the net while serving\n",
    "- missing a serve\n",
    "- hitting the ball out of bounds of its own court\n",
    "- double hitting\n",
    "- letting the ball touch its own court while sending\n",
    "- letting the ball touch its own court more than once while receiving\n",
    "\n",
    "\n",
    "The observation space consists of a stack of 3 consecutive observation frames, where each frame is a vector of `9` variables corresponding to the position, velocity, and rotation of the ball and rackets.\n",
    "\n",
    "Each agent receives its own, local observation. `3` continuous actions are available, corresponding to movement toward (or away from) the net, jumping, and rotating the racket.\n",
    "\n",
    "The task is episodic, and ends when one agent wins a game. The environment is considered solved, when the agents get an average score of `5.0` (over `100` consecutive episodes, after taking the maximum over both agents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Modified Reward Model \n",
    "\n",
    "The reward model for this environment has been modified from the [original reward model](https://github.com/Unity-Technologies/ml-agents/blob/com.unity.ml-agents_1.0.7/Project/Assets/ML-Agents/Examples/Tennis/Scripts/HitWall.cs#L42)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "import pprint\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display as Display\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display as display\n",
    "display = display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Method, Parameters and Style-sheets for Rendering the Scene inside the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper button.btn.btn-default,\n",
    ".output_wrapper .ui-dialog-titlebar,\n",
    ".output_wrapper .mpl-message {\n",
    "    display: none;\n",
    "}\n",
    ".output_wrapper .ui-dialog-titlebar + div {\n",
    "    border: none !important;\n",
    "    overflow: under !important\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def animate_frames(frames):\n",
    "    'function to animate a list of frames'\n",
    "    def display_animation(anim):\n",
    "        plt.close(anim._fig)\n",
    "        return HTML(anim.to_jshtml())\n",
    "    plt.axis('off')\n",
    "    cmap = None if len(frames[0].shape) == 3 else 'Greys'\n",
    "    patch = plt.imshow(frames[0], cmap=cmap, aspect='auto')\n",
    "    plt.gcf().set_size_inches(display_width * IMAGE_SCALE / DPI, display_height * IMAGE_SCALE / DPI)\n",
    "    fanim = animation.FuncAnimation(plt.gcf(), lambda x: patch.set_data(frames[x]), frames = len(frames), interval=ANIMATION_INTERVAL)\n",
    "    Display(display_animation(fanim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "DPI                                          = 96     # https://www.infobyip.com/detectmonitordpi.php\n",
    "IMAGE_SCALE                                  = 1.4    # large scale == more buffer space == slow to render\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128 # animation buffer size\n",
    "ANIMATION_INTERVAL                           = 60     # delay between frames in ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MODEL_FILE               = './models/maddpg-tennis.pt'\n",
    "\n",
    "ENVIRONMENT              = './Unity ML Agent/Tennis/Linux/Tennis.x86_64'\n",
    "\n",
    "CAMERA_AGENT_NAME_PREFIX = 'Camera'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=ENVIRONMENT, seed=1, side_channels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Examine the State and Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "agent_info = {}\n",
    "camera_agent, display_width, display_height = None, None, None\n",
    "for behavior_name in env.behavior_specs.keys():\n",
    "    print('\\nBehavior Name: {}'.format(behavior_name))\n",
    "    print('State Space: {}'.format(env.behavior_specs[behavior_name].observation_specs))\n",
    "    print('Action Space: {}'.format(env.behavior_specs[behavior_name].action_spec))\n",
    "    if not behavior_name.startswith(CAMERA_AGENT_NAME_PREFIX):\n",
    "        agent_info[behavior_name] = {\n",
    "            'state_size': env.behavior_specs[behavior_name].observation_specs[0].shape[0],\n",
    "            'continuous_action_size': env.behavior_specs[behavior_name].action_spec.continuous_size,\n",
    "            'discrete_action_n_branches': env.behavior_specs[behavior_name].action_spec.discrete_size,\n",
    "            'discrete_action_branches': env.behavior_specs[behavior_name].action_spec.discrete_branches\n",
    "        }\n",
    "    else:\n",
    "        camera_agent = behavior_name\n",
    "        display_height = env.behavior_specs[behavior_name].observation_specs[0].shape[0]\n",
    "        display_width = env.behavior_specs[behavior_name].observation_specs[0].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### IDs and State Action Spaces of Different Agents in a Multi Agent Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(agent_info)\n",
    "\n",
    "\n",
    "num_agents = len(agent_info)\n",
    "state_size = list(agent_info.values())[0]['state_size']\n",
    "action_size = list(agent_info.values())[0]['continuous_action_size']\n",
    "\n",
    "print('\\nNumber of Agents: {}'.format(num_agents))\n",
    "print('State Size: {}'.format(state_size))\n",
    "print('Action Size: {}'.format(action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### `Teams` and `Agents`\n",
    "\n",
    "There can be multiple agent-groups (`teams`) in the environment and each team can have multiple `agents` sharing the same behaviour (state, action, and reward space). However, teams usually follow different behaviour.\n",
    "\n",
    "To see all the available `teams` and the `agents` (IDs) in each team, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for agent_group in agent_info:\n",
    "    decision_steps, _ = env.get_steps(agent_group)\n",
    "    print('Team Name: {}\\tAgent IDs: {}'.format(agent_group, decision_steps.agent_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Take Random Actions in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def random_action(agent_params):\n",
    "    _continuous = np.random.uniform(low=-1.0, high=1.0, size=(1, agent_params['continuous_action_size']))\n",
    "    _discrete = np.zeros((1, agent_params['discrete_action_n_branches']), dtype=np.int32)\n",
    "    for branch_idx in range(agent_params['discrete_action_n_branches']):\n",
    "        _discrete = np.column_stack([\n",
    "            np.random.randint(0, agent_params['discrete_action_branches'][branch_idx], size=(1), dtype=np.int32)\n",
    "            for branch_idx in range(agent_params['discrete_action_n_branches'])\n",
    "        ])\n",
    "    return ActionTuple(continuous=_continuous, discrete=_discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "RENDER_REAL_TIME = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6,
     10
    ],
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if RENDER_REAL_TIME:\n",
    "    %matplotlib notebook\n",
    "    fig = plt.figure()\n",
    "    plt.gcf().set_size_inches(display_width * IMAGE_SCALE / DPI, display_height * IMAGE_SCALE / DPI)\n",
    "    plt.axis('off')\n",
    "    img = plt.imshow(np.zeros((display_height, display_width, 3)))\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    frames = []\n",
    "\n",
    "for episode_i in range(2):\n",
    "    env.reset()                                                           # reset the environment\n",
    "    score, terminated, experience = {}, {}, {}\n",
    "    while True:\n",
    "        for behavior_name, agent_params in agent_info.items():            # for every agent-group in the environment\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name) # get agents' status in the agent-group\n",
    "            if behavior_name not in score:\n",
    "                score[behavior_name] = {}\n",
    "                for i in np.concatenate((decision_steps.agent_id, terminal_steps.agent_id)):\n",
    "                    score[behavior_name][i] = 0\n",
    "            if behavior_name not in terminated:\n",
    "                terminated[behavior_name] = {}\n",
    "                for i in np.concatenate((decision_steps.agent_id, terminal_steps.agent_id)):\n",
    "                    terminated[behavior_name][i] = False\n",
    "            if behavior_name not in experience:\n",
    "                experience[behavior_name] = {}\n",
    "                for i in np.concatenate((decision_steps.agent_id, terminal_steps.agent_id)):\n",
    "                    experience[behavior_name][i] = {\n",
    "                        'state': None,\n",
    "                        'action': None,\n",
    "                        'reward': None,\n",
    "                        'next_state': None\n",
    "                    }\n",
    "            for agent_id in decision_steps.agent_id:                          # for every agent in the agent-group                \n",
    "                state = decision_steps[agent_id].obs                          # get the initial state for the agent\n",
    "\n",
    "                # select an action based on the policy\n",
    "                # policy.choose(behavior_name, agent_id, state)\n",
    "                action = random_action(agent_params)                          # select an action\n",
    "\n",
    "                experience[behavior_name][agent_id]['state'] = state\n",
    "                experience[behavior_name][agent_id]['action'] = action\n",
    "                env.set_action_for_agent(behavior_name, agent_id, action)     # send the action to the agent\n",
    "                # env.set_actions(behavior_name, action)                      # send the action to the agent-group\n",
    "        if camera_agent:                                                      # render the screen\n",
    "            decision_steps, _ = env.get_steps(camera_agent)\n",
    "            if (len(decision_steps.agent_id)):\n",
    "                camera_agent_id = decision_steps.agent_id[0]\n",
    "                image = decision_steps[camera_agent_id].obs\n",
    "                if RENDER_REAL_TIME:\n",
    "                    img.set_data(image[0])\n",
    "                    fig.canvas.draw()\n",
    "                else:\n",
    "                    frames.append(image[0])\n",
    "                    \n",
    "\n",
    "        env.step()                                                            # one step through the environment\n",
    "\n",
    "        for behavior_name in agent_info:                                      # for every agent-group in the environment\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)     # get agents' status in the agent-group    \n",
    "            for agent_id in decision_steps.agent_id:\n",
    "                reward = decision_steps[agent_id].reward                      # get the reward\n",
    "                next_state = decision_steps[agent_id].obs                     # get the next state\n",
    "                score[behavior_name][agent_id] += reward\n",
    "                experience[behavior_name][agent_id]['reward'] = reward\n",
    "                experience[behavior_name][agent_id]['next_state'] = next_state\n",
    "            for agent_id in terminal_steps.agent_id:                          # see if episode finished for an agent\n",
    "                reward = terminal_steps[agent_id].reward\n",
    "                score[behavior_name][agent_id] += reward\n",
    "                experience[behavior_name][agent_id]['reward'] = reward\n",
    "                experience[behavior_name][agent_id]['next_state'] = None\n",
    "                terminated[behavior_name][agent_id] = True\n",
    "                if terminal_steps[agent_id].interrupted:\n",
    "                    print('agent #{} in the agent-group \"{}\" has reached the maximum number of steps in the episode'.format(\n",
    "                        agent_id, behavior_name))\n",
    "                    \n",
    "        if camera_agent:                                                      # render the screen\n",
    "            decision_steps, _ = env.get_steps(camera_agent)\n",
    "            if (len(decision_steps.agent_id)):\n",
    "                camera_agent_id = decision_steps.agent_id[0]\n",
    "                image = decision_steps[camera_agent_id].obs\n",
    "                if RENDER_REAL_TIME:\n",
    "                    img.set_data(image[0])\n",
    "                    fig.canvas.draw()\n",
    "                else:\n",
    "                    frames.append(image[0])\n",
    "\n",
    "        # train the RL agent with the experience tuple\n",
    "        # agent.step(experience)\n",
    "\n",
    "        # if EVERY agent in EVERY agent-group is done\n",
    "        if np.asarray([np.asarray(list(agent_group.values())).all() for agent_group in terminated.values()]).all():\n",
    "            break\n",
    "\n",
    "    print(score)\n",
    "\n",
    "if not RENDER_REAL_TIME: animate_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Hyperparameters of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED         = 6\n",
    "\n",
    "NUM_AGENTS   = num_agents\n",
    "ACTION_SIZE  = action_size\n",
    "STATE_SIZE   = state_size\n",
    "\n",
    "BUFFER_SIZE  = 5120\n",
    "BATCH_SIZE   = 1024\n",
    "GAMMA        = 0.99\n",
    "TAU          = 3e-3\n",
    "\n",
    "NOISE_MU     = 0.0\n",
    "NOISE_THETA  = 0.15\n",
    "NOISE_SIGMA  = 0.1\n",
    "\n",
    "ACTOR_LR     = 1e-3\n",
    "CRITIC_LR    = 2e-3\n",
    "\n",
    "TARGET_SCORE = 5.0    # score for which the environment is consider to be Solved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Noise Model for Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    def __init__(self, size, mu, theta, sigma):\n",
    "        self.state = None\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([random.random() for _ in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Replay:\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        self.action_size = action_size\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.buffer, k=self.batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### The Actor-Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     5,
     25
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return -lim, lim\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1_units=256, fc2_units=128):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.bn2 = nn.BatchNorm1d(fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.bn1(F.relu(self.fc1(state)))\n",
    "        x = self.bn2(F.relu(self.fc2(x)))\n",
    "        return torch.tanh(self.fc3(x))\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1_units=256, fc2_units=128):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units + action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = self.bn1(F.relu(self.fc1(state)))\n",
    "        x = F.relu(self.fc2(torch.cat((x, action), dim=1)))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Multi Agent DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     16,
     35,
     56,
     95,
     100,
     104
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class MultiAgentDDPG:\n",
    "    def __init__(self):\n",
    "        self.actor = Actor(STATE_SIZE, ACTION_SIZE).to(device)\n",
    "        self.target_actor = Actor(STATE_SIZE, ACTION_SIZE).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=ACTOR_LR)\n",
    "\n",
    "        self.critic = Critic(STATE_SIZE * NUM_AGENTS, ACTION_SIZE * NUM_AGENTS).to(device)\n",
    "        self.target_critic = Critic(STATE_SIZE * NUM_AGENTS, ACTION_SIZE * NUM_AGENTS).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=CRITIC_LR)\n",
    "\n",
    "        self.noise = OUNoise(ACTION_SIZE, mu=NOISE_MU, theta=NOISE_THETA, sigma=NOISE_SIGMA)\n",
    "        self.replay = Replay(ACTION_SIZE, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE)\n",
    "\n",
    "        self.hard_update(self.target_actor, self.actor)\n",
    "        self.hard_update(self.target_critic, self.critic)\n",
    "\n",
    "    def act(self, agent_params, states, noise=True):\n",
    "        state = torch.from_numpy(np.asarray(states)).float().to(device)\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state).cpu().numpy()\n",
    "        self.actor.train()\n",
    "        if noise:\n",
    "            action += self.noise.sample()\n",
    "        _continuous = np.clip(action, -1, 1)\n",
    "        \n",
    "        # dummy placeholder for unused discrete action space\n",
    "        _discrete = np.zeros((1, agent_params['discrete_action_n_branches']), dtype=np.int32)\n",
    "        for branch_idx in range(agent_params['discrete_action_n_branches']):\n",
    "            _discrete = np.column_stack([\n",
    "                np.random.randint(0, agent_params['discrete_action_branches'][branch_idx], size=(1), dtype=np.int32)\n",
    "                for branch_idx in range(agent_params['discrete_action_n_branches'])\n",
    "            ])\n",
    "        return ActionTuple(continuous=_continuous, discrete=_discrete)\n",
    "\n",
    "    def step(self, experience):\n",
    "        teams = list(experience.keys())\n",
    "        full_state = np.asarray([exp['state'][0] for team in teams for exp in experience[team].values()])\n",
    "        next_full_state = np.asarray([exp['next_state'][0] for team in teams for exp in experience[team].values()])\n",
    "        full_action = np.asarray([exp['action'].continuous[0] for team in teams for exp in experience[team].values()])\n",
    "        for team in teams:\n",
    "            for exp in experience[team].values():\n",
    "                self.replay.add((\n",
    "                    exp['state'][0],\n",
    "                    full_state,\n",
    "                    exp['action'].continuous[0],\n",
    "                    full_action,\n",
    "                    exp['reward'],\n",
    "                    exp['next_state'][0],\n",
    "                    next_full_state,\n",
    "                    exp['done']\n",
    "                ))\n",
    "\n",
    "        if len(self.replay) > self.replay.batch_size:\n",
    "            self.learn()\n",
    "\n",
    "    def learn(self):\n",
    "        # sample a batch of transitions from the replay buffer\n",
    "        transitions = self.replay.sample()\n",
    "        states, full_state, actions, full_actions, rewards, next_states, next_full_state, dones = self.transpose_to_tensor(transitions)        \n",
    "\n",
    "        # -------------------- update critic --------------------\n",
    "        with torch.no_grad():\n",
    "            target_next_actions = [self.target_actor(next_full_state[:, i, :]) for i in range(NUM_AGENTS)]\n",
    "            target_next_actions = torch.cat(target_next_actions, dim=1)\n",
    "            next_full_state = next_full_state.reshape((next_full_state.shape[0], -1))\n",
    "            target_next_q_values = self.target_critic(next_full_state.to(device), target_next_actions.to(device))\n",
    "\n",
    "        td_target = rewards.view(-1,1) + GAMMA * target_next_q_values * (1 - dones.view(-1,1))\n",
    "\n",
    "        # compute Q values for the current states (for all agents) and actions (for all agents) using the critic\n",
    "        full_actions = full_actions.reshape((full_actions.shape[0], -1))\n",
    "        current_q_values = self.critic(full_state.reshape((full_state.shape[0], -1)).to(device), full_actions.to(device))\n",
    "\n",
    "        # compute and minimize the critic loss\n",
    "        critic_loss = F.mse_loss(current_q_values, td_target.detach())\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # -------------------- update actor --------------------\n",
    "        actions = [self.actor(full_state[:, i, :]) for i in range(NUM_AGENTS)]\n",
    "        actions = torch.cat(actions, dim=1)\n",
    "        states = full_state.reshape((full_state.shape[0], -1))\n",
    "        actor_loss = - self.critic(states.to(device), actions.to(device)).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # update target critic and actor models\n",
    "        self.soft_update(self.target_actor, self.actor, TAU)\n",
    "        self.soft_update(self.target_critic, self.critic, TAU)\n",
    "        \n",
    "    def transpose_to_tensor(self, tuples):\n",
    "        def to_tensor(x):\n",
    "            return torch.tensor(x, dtype=torch.float).to(device)\n",
    "        return list(map(to_tensor, zip(*tuples)))\n",
    "\n",
    "    def hard_update(self, target_model, source_model):\n",
    "        for target_param, param in zip(target_model.parameters(), source_model.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "    def soft_update(self, target_model, source_model, tau):\n",
    "        for target_param, online_param in zip(target_model.parameters(), source_model.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - tau) + online_param.data * tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Fault Tolerant Training Function\n",
    "\n",
    "Training loop can be interrupted in between and be resumed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     36,
     50,
     52,
     56,
     60,
     70,
     82,
     92,
     110
    ],
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(max_episodes=2000, print_every=50, save_every=100, render_every=50):\n",
    "    '''\n",
    "    PARAMS\n",
    "    ======\n",
    "        render_every (int): render an episode after every n such episodes (-1 for no rendering) \n",
    "    '''\n",
    "    \n",
    "    agent = MultiAgentDDPG()\n",
    "    \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    start_episode, solved = 0, False\n",
    "    scores, mean_scores = [], []\n",
    "\n",
    "    if os.path.isfile(MODEL_FILE):\n",
    "        # load training checkpoints from file\n",
    "        map_location = (lambda storage, loc: storage.cuda()) if torch.cuda.is_available() else 'cpu'\n",
    "        checkpoint = torch.load(MODEL_FILE, map_location=map_location)\n",
    "        agent.actor.load_state_dict(checkpoint['actor'])\n",
    "        agent.target_actor.load_state_dict(checkpoint['target_actor'])\n",
    "        agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n",
    "        agent.critic.load_state_dict(checkpoint['critic'])\n",
    "        agent.target_critic.load_state_dict(checkpoint['target_critic'])\n",
    "        agent.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n",
    "        scores = checkpoint['scores']\n",
    "        mean_scores = checkpoint['mean_scores']\n",
    "        start_episode = checkpoint['episode']\n",
    "        solved = checkpoint['solved']\n",
    " \n",
    "    if solved:\n",
    "        return scores, mean_scores\n",
    "    \n",
    "    agent.actor.train()\n",
    "    agent.critic.train()\n",
    "    \n",
    "    env = UnityEnvironment(file_name=ENVIRONMENT, seed=1, side_channels=[])\n",
    "    \n",
    "    if render_every != -1:\n",
    "        %matplotlib notebook\n",
    "        fig = plt.figure()\n",
    "        plt.gcf().set_size_inches(display_width * IMAGE_SCALE / DPI, display_height * IMAGE_SCALE / DPI)\n",
    "        plt.axis('off')\n",
    "        img = plt.imshow(np.zeros((display_height, display_width, 3)))\n",
    "\n",
    "    try:\n",
    "        for episode in range(start_episode, start_episode + max_episodes):\n",
    "            score = np.zeros(NUM_AGENTS)\n",
    "            env.reset()                                                           # reset the environment\n",
    "            score, terminated, experience = {}, {}, {}\n",
    "            if (episode + 1) % render_every == 0: frames = []\n",
    "            while True:\n",
    "                for behavior_name, agent_params in agent_info.items():            # for every agent-group in the environment\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name) # get agents' status in the agent-group\n",
    "                    if behavior_name not in score:\n",
    "                        score[behavior_name] = {}\n",
    "                        for i in np.concatenate((decision_steps.agent_id, terminal_steps.agent_id)):\n",
    "                            score[behavior_name][i] = 0\n",
    "                    if behavior_name not in terminated:\n",
    "                        terminated[behavior_name] = {}\n",
    "                        for i in np.concatenate((decision_steps.agent_id, terminal_steps.agent_id)):\n",
    "                            terminated[behavior_name][i] = False\n",
    "                    if behavior_name not in experience:\n",
    "                        experience[behavior_name] = {}\n",
    "                        for i in np.concatenate((decision_steps.agent_id, terminal_steps.agent_id)):\n",
    "                            experience[behavior_name][i] = {\n",
    "                                'state': None,\n",
    "                                'action': None,\n",
    "                                'reward': None,\n",
    "                                'next_state': None,\n",
    "                                'done': False\n",
    "                            }\n",
    "                    for agent_id in decision_steps.agent_id:                          # for every agent in the agent-group                \n",
    "                        state = decision_steps[agent_id].obs                          # get the initial state for the agent\n",
    "\n",
    "                        # select an action based on the policy\n",
    "                        # policy.choose(behavior_name, agent_id, state)\n",
    "                        action = agent.act(agent_params, state)                       # select an action\n",
    "\n",
    "                        experience[behavior_name][agent_id]['state'] = state\n",
    "                        experience[behavior_name][agent_id]['action'] = action\n",
    "                        env.set_action_for_agent(behavior_name, agent_id, action)     # send the action to the agent\n",
    "                        # env.set_actions(behavior_name, action)                  # send the action to the agent-group\n",
    "\n",
    "                if camera_agent and (render_every != -1) and ((episode + 1) % render_every == 0):\n",
    "                    decision_steps, _ = env.get_steps(camera_agent)\n",
    "                    if (len(decision_steps.agent_id)):\n",
    "                        camera_agent_id = decision_steps.agent_id[0]\n",
    "                        image = decision_steps[camera_agent_id].obs\n",
    "                        img.set_data(image[0])\n",
    "                        fig.canvas.draw()\n",
    "\n",
    "                env.step()                                                         # one step through the environment\n",
    "\n",
    "                for behavior_name in agent_info:                                   # for every agent-group in the environment\n",
    "                    decision_steps, terminal_steps = env.get_steps(behavior_name)     # get agents' status in the agent-group    \n",
    "                    for agent_id in decision_steps.agent_id:\n",
    "                        reward = decision_steps[agent_id].reward                      # get the reward\n",
    "                        next_state = decision_steps[agent_id].obs                     # get the next state\n",
    "                        score[behavior_name][agent_id] += reward\n",
    "                        experience[behavior_name][agent_id]['reward'] = reward\n",
    "                        experience[behavior_name][agent_id]['next_state'] = next_state\n",
    "                    for agent_id in terminal_steps.agent_id:                          # see if episode finished for an agent\n",
    "                        reward = terminal_steps[agent_id].reward\n",
    "                        score[behavior_name][agent_id] += reward\n",
    "                        experience[behavior_name][agent_id]['reward'] = reward\n",
    "                        experience[behavior_name][agent_id]['next_state'] = experience[behavior_name][agent_id]['state']\n",
    "                        experience[behavior_name][agent_id]['done'] = True\n",
    "                        terminated[behavior_name][agent_id] = True\n",
    "                        if terminal_steps[agent_id].interrupted:\n",
    "                            print('agent #{} in the agent-group \"{}\" has reached the maximum number of steps in the episode'.format(agent_id, behavior_name)) \n",
    "\n",
    "                if camera_agent and (render_every != -1) and ((episode + 1) % render_every == 0):\n",
    "                    decision_steps, _ = env.get_steps(camera_agent)\n",
    "                    if (len(decision_steps.agent_id)):\n",
    "                        camera_agent_id = decision_steps.agent_id[0]\n",
    "                        image = decision_steps[camera_agent_id].obs\n",
    "                        img.set_data(image[0])\n",
    "                        fig.canvas.draw()\n",
    "\n",
    "                # train the RL agent with the experience tuple\n",
    "                agent.step(experience)\n",
    "\n",
    "                # if EVERY agent in ANY agent-group is done\n",
    "                if np.asarray([np.asarray(list(agent_group.values())).all() for agent_group in terminated.values()]).any():\n",
    "                    break\n",
    "\n",
    "            agent_scores = [sum([agent_score for agent_score in team_score.values()]) for team_score in score.values()]\n",
    "            score = max(agent_scores)\n",
    "            scores.append(score)\n",
    "            scores_deque.append(score)\n",
    "            mean_scores.append(np.mean(scores_deque))\n",
    "            agent_scores = list(map(lambda x: '{:.4f}'.format(x), agent_scores))\n",
    "\n",
    "            print('\\r' + ' ' * 120, end='')\n",
    "            print('\\rEpisode {:04d}    Average Score: {:.4f}    Current Max Score: {:.4f}    Current Scores: {}'.format(episode + 1, mean_scores[-1], score, agent_scores), end='')\n",
    "            if (episode + 1) % print_every == 0:\n",
    "                print('\\r' + ' ' * 120, end='')\n",
    "                print('\\rEpisode {:04d}    Average Score: {:.4f}'.format(episode + 1, mean_scores[-1]))\n",
    "\n",
    "            if (episode + 1) % save_every == 0 or mean_scores[-1] >= TARGET_SCORE:\n",
    "                torch.save({\n",
    "                    'episode': episode + 1,\n",
    "                    'actor': agent.actor.state_dict(),\n",
    "                    'target_actor': agent.target_actor.state_dict(),\n",
    "                    'actor_optimizer': agent.actor_optimizer.state_dict(),\n",
    "                    'critic': agent.critic.state_dict(),\n",
    "                    'target_critic': agent.target_critic.state_dict(),\n",
    "                    'critic_optimizer': agent.critic_optimizer.state_dict(),\n",
    "                    'scores': scores,\n",
    "                    'mean_scores': mean_scores,\n",
    "                    'solved': True if mean_scores[-1] >= TARGET_SCORE else False\n",
    "                }, MODEL_FILE)\n",
    "                if mean_scores[-1] >= TARGET_SCORE:\n",
    "                    print('\\nEnvironment solved in {:04d} episodes!\\tAverage Score: {:.4f}'.format(episode + 1, mean_scores[-1]))\n",
    "                    break\n",
    "\n",
    "        env.close()\n",
    "        return scores, mean_scores\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\nTraining Interrupted')\n",
    "        env.close()\n",
    "        return scores, mean_scores\n",
    "    \n",
    "    env.close()    \n",
    "    return scores, mean_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "scores, mean_scores = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Plot Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(1, len(scores) + 1)\n",
    "ax.plot(x, scores)\n",
    "ax.plot(x, mean_scores)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "### Load the Trained Agents for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=ENVIRONMENT, seed=1, side_channels=[])\n",
    "\n",
    "agent = MultiAgentDDPG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# load the weights from file\n",
    "map_location = (lambda storage, loc: storage.cuda()) if torch.cuda.is_available() else 'cpu'\n",
    "checkpoint = torch.load(MODEL_FILE, map_location=map_location)\n",
    "agent.actor.load_state_dict(checkpoint['actor']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Start the Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_GAMES = 10\n",
    "N_GAMES_TO_RENDER = 4\n",
    "RENDER_REAL_TIME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     6,
     10
    ],
    "deletable": false,
    "editable": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if RENDER_REAL_TIME:\n",
    "    %matplotlib notebook\n",
    "    fig = plt.figure()\n",
    "    plt.gcf().set_size_inches(display_width * IMAGE_SCALE / DPI, display_height * IMAGE_SCALE / DPI)\n",
    "    plt.axis('off')\n",
    "    img = plt.imshow(np.zeros((display_height, display_width, 3)))\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    frames = []\n",
    "render_list = np.random.choice(range(N_GAMES), size=N_GAMES_TO_RENDER, replace=False)\n",
    "for episode in range(N_GAMES):\n",
    "    env.reset()\n",
    "    terminated = {}\n",
    "    while True:\n",
    "        for behavior_name, agent_params in agent_info.items():\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            if behavior_name not in terminated:\n",
    "                terminated[behavior_name] = {}\n",
    "                for i in np.concatenate((decision_steps.agent_id, terminal_steps.agent_id)):\n",
    "                    terminated[behavior_name][i] = False\n",
    "            for agent_id in decision_steps.agent_id:\n",
    "                state = decision_steps[agent_id].obs\n",
    "                action = agent.act(agent_params, state)\n",
    "                env.set_action_for_agent(behavior_name, agent_id, action)\n",
    "        if camera_agent and (RENDER_REAL_TIME or (episode in render_list)):\n",
    "            decision_steps, _ = env.get_steps(camera_agent)\n",
    "            if (len(decision_steps.agent_id)):\n",
    "                camera_agent_id = decision_steps.agent_id[0]\n",
    "                image = decision_steps[camera_agent_id].obs\n",
    "                if RENDER_REAL_TIME:\n",
    "                    img.set_data(image[0])\n",
    "                    fig.canvas.draw()\n",
    "                else:\n",
    "                    frames.append(image[0])\n",
    "        env.step()\n",
    "        for behavior_name in agent_info:\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "            for agent_id in terminal_steps.agent_id:\n",
    "                terminated[behavior_name][agent_id] = True\n",
    "                if terminal_steps[agent_id].interrupted:\n",
    "                    print('agent #{} in the agent-group \"{}\" has reached the maximum number of steps in the episode'.format(agent_id, behavior_name))\n",
    "        if camera_agent and (RENDER_REAL_TIME or (episode in render_list)):\n",
    "            decision_steps, _ = env.get_steps(camera_agent)\n",
    "            if (len(decision_steps.agent_id)):\n",
    "                camera_agent_id = decision_steps.agent_id[0]\n",
    "                image = decision_steps[camera_agent_id].obs\n",
    "                if RENDER_REAL_TIME:\n",
    "                    img.set_data(image[0])\n",
    "                    fig.canvas.draw()\n",
    "                else:\n",
    "                    frames.append(image[0])\n",
    "        if np.asarray([np.asarray(list(agent_group.values())).all() for agent_group in terminated.values()]).any():\n",
    "            break\n",
    "%matplotlib inline\n",
    "if not RENDER_REAL_TIME: animate_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "Next: [Additional Materials](./Additional.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

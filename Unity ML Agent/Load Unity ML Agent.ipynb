{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Unity ML Agent\n",
    "\n",
    "This notebook demonstrates the use of the Unity ML-Agents environment.\n",
    "\n",
    "### Set up an Environment\n",
    "\n",
    "This notebook was tested with `Unity Hub Version 2.4.3`, `Unity Version 2020.3.4f1`, `Unity ML Agent Version 1.8.0-preview` and python `mlagents-envs` version `0.25.1` (`Release 16`).\n",
    "\n",
    "Follow the [Unity ML-Agents Documentation](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Readme.md) to set up an Unity ML-Agent environment from scratch.\n",
    "\n",
    "### Get Images from Unity Environment for Rendering in Notebook\n",
    "\n",
    "To get the images of the unity scenes, add [these 4 scripts](https://i.imgur.com/nyKG4Yk.png) to the `Main Camera`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pprint\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import display as Display\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display as display\n",
    "display = display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "from mlagents_envs.base_env import ActionTuple\n",
    "from mlagents_envs.environment import UnityEnvironment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Method, Parameters and Style-sheets for Rendering the Scene inside the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper button.btn.btn-default,\n",
    ".output_wrapper .ui-dialog-titlebar,\n",
    ".output_wrapper .mpl-message {\n",
    "    display: none;\n",
    "}\n",
    ".output_wrapper .ui-dialog-titlebar + div {\n",
    "    border: none !important;\n",
    "    overflow: under !important\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def animate_frames(frames):\n",
    "    'function to animate a list of frames'\n",
    "    def display_animation(anim):\n",
    "        plt.close(anim._fig)\n",
    "        return HTML(anim.to_jshtml())\n",
    "    plt.axis('off')\n",
    "    cmap = None if len(frames[0].shape) == 3 else 'Greys'\n",
    "    patch = plt.imshow(frames[0], cmap=cmap, aspect='auto')\n",
    "    plt.gcf().set_size_inches(display_width * IMAGE_SCALE / DPI, display_height * IMAGE_SCALE / DPI)\n",
    "    fanim = animation.FuncAnimation(plt.gcf(), lambda x: patch.set_data(frames[x]), frames = len(frames), interval=ANIMATION_INTERVAL)\n",
    "    Display(display_animation(fanim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "DPI                                          = 96     # https://www.infobyip.com/detectmonitordpi.php\n",
    "IMAGE_SCALE                                  = 1.4    # large scale == more buffer space == slow to render\n",
    "matplotlib.rcParams['animation.embed_limit'] = 2**128 # animation buffer size\n",
    "ANIMATION_INTERVAL                           = 60     # delay between frames in ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Next, start the environment!\n",
    "\n",
    "**Before running the code cell below**, change the `ENVIRONMENT` parameter to match the location of the Unity environment: `path/to/<UNITY-ML-APP>`\n",
    "\n",
    "Also, the `CAMERA_AGENT_NAME_PREFIX` parameter should match the `Behavior Name` corresponding to the `Main Camera`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT = './RollerBall/Linux_Headless/RollerBall.x86_64'\n",
    "\n",
    "CAMERA_AGENT_NAME_PREFIX = 'Camera'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Load the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=ENVIRONMENT, seed=1, side_channels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent navigating through an environment. At each time step, it has five actions to choose from:\n",
    "- `0` - no action\n",
    "- `1` - move up \n",
    "- `2` - move down\n",
    "- `3` - move left\n",
    "- `4` - move right\n",
    "\n",
    "The state space has `8` dimensions and contains the agent's position, target position, agent's velocity along x and y direction.\n",
    "\n",
    "A reward of `+1.0` is provided for hitting the target, a penalty of `-2.0` is given for falling off the floor, and for each time step spent navigating, a penalty of `-0.01` is given.\n",
    "\n",
    "There is also a `Behavior Name` that looks like `Camera?*` and it corresponding to the images of the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "agent_info = {}\n",
    "camera_agent, display_width, display_height = None, None, None\n",
    "for behavior_name in env.behavior_specs.keys():\n",
    "    print('\\nBehavior Name: {}'.format(behavior_name))\n",
    "    print('State Space: {}'.format(env.behavior_specs[behavior_name].observation_specs))\n",
    "    print('Action Space: {}'.format(env.behavior_specs[behavior_name].action_spec))\n",
    "    if not behavior_name.startswith(CAMERA_AGENT_NAME_PREFIX):\n",
    "        agent_info[behavior_name] = {\n",
    "            'state_size': env.behavior_specs[behavior_name].observation_specs[0].shape[0],\n",
    "            'continuous_action_size': env.behavior_specs[behavior_name].action_spec.continuous_size,\n",
    "            'discrete_action_n_branches': env.behavior_specs[behavior_name].action_spec.discrete_size,\n",
    "            'discrete_action_branches': env.behavior_specs[behavior_name].action_spec.discrete_branches\n",
    "        }\n",
    "    else:\n",
    "        camera_agent = behavior_name\n",
    "        display_height = env.behavior_specs[behavior_name].observation_specs[0].shape[0]\n",
    "        display_width = env.behavior_specs[behavior_name].observation_specs[0].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### IDs and State Action Spaces the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "pprint.pprint(agent_info)\n",
    "\n",
    "\n",
    "num_agents = len(agent_info)\n",
    "state_size = list(agent_info.values())[0]['state_size']\n",
    "action_size = list(agent_info.values())[0]['discrete_action_branches'][0]\n",
    "\n",
    "print('\\nNumber of Agents: {}'.format(num_agents))\n",
    "print('State Size: {}'.format(state_size))\n",
    "print('Action Size: {}'.format(action_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### `Teams` and `Agents`\n",
    "\n",
    "There can be multiple agent-groups (`teams`) in the environment and each team can have multiple `agents` sharing the same behaviour (state, action, and reward space). However, teams usually follow different behaviour.\n",
    "\n",
    "To see all the available `teams` and the `agents` (IDs) in each team, execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for agent_group in agent_info:\n",
    "    decision_steps, _ = env.get_steps(agent_group)\n",
    "    print('Team Name: {}\\tAgent IDs: {}'.format(agent_group, decision_steps.agent_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Take random actions in the environment\n",
    "\n",
    "In the next code cell, we will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, we will watch the agent's performance, if it selects an action (uniformly) at random with each time step. A window should pop up that allows us to observe the agent, as it moves through the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def random_action(agent_params):\n",
    "    _continuous = np.random.uniform(low=-1.0, high=1.0, size=(1, agent_params['continuous_action_size']))\n",
    "    _discrete = np.zeros((1, agent_params['discrete_action_n_branches']), dtype=np.int32)\n",
    "    for branch_idx in range(agent_params['discrete_action_n_branches']):\n",
    "        _discrete = np.column_stack([\n",
    "            np.random.randint(0, agent_params['discrete_action_branches'][branch_idx], size=(1), dtype=np.int32)\n",
    "            for branch_idx in range(agent_params['discrete_action_n_branches'])\n",
    "        ])\n",
    "    return ActionTuple(continuous=_continuous, discrete=_discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RENDER_REAL_TIME = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     10
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "if RENDER_REAL_TIME:\n",
    "    %matplotlib notebook\n",
    "    fig = plt.figure()\n",
    "    plt.gcf().set_size_inches(display_width * IMAGE_SCALE / DPI, display_height * IMAGE_SCALE / DPI)\n",
    "    plt.axis('off')\n",
    "    img = plt.imshow(np.zeros((display_height, display_width, 3)))\n",
    "else:\n",
    "    %matplotlib inline\n",
    "    frames = []\n",
    "\n",
    "for episode_i in range(2):\n",
    "    env.reset()                                                           # reset the environment\n",
    "    score, terminated, experience = {}, {}, {}\n",
    "    while True:\n",
    "        for behavior_name, agent_params in agent_info.items():            # for every agent-group in the environment\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name) # get agents' status in the agent-group\n",
    "            if behavior_name not in score:\n",
    "                score[behavior_name] = {}\n",
    "                for i in np.concatenate((decision_steps.agent_id, terminal_steps.agent_id)):\n",
    "                    score[behavior_name][i] = 0\n",
    "            if behavior_name not in terminated:\n",
    "                terminated[behavior_name] = {}\n",
    "                for i in np.concatenate((decision_steps.agent_id, terminal_steps.agent_id)):\n",
    "                    terminated[behavior_name][i] = False\n",
    "            if behavior_name not in experience:\n",
    "                experience[behavior_name] = {}\n",
    "                for i in np.concatenate((decision_steps.agent_id, terminal_steps.agent_id)):\n",
    "                    experience[behavior_name][i] = {\n",
    "                        'state': None,\n",
    "                        'action': None,\n",
    "                        'reward': None,\n",
    "                        'next_state': None\n",
    "                    }\n",
    "            for agent_id in decision_steps.agent_id:                          # for every agent in the agent-group                \n",
    "                state = decision_steps[agent_id].obs                          # get the initial state for the agent\n",
    "\n",
    "                # select an action based on the policy\n",
    "                # policy.choose(behavior_name, agent_id, state)\n",
    "                action = random_action(agent_params)                          # select an action\n",
    "\n",
    "                experience[behavior_name][agent_id]['state'] = state\n",
    "                experience[behavior_name][agent_id]['action'] = action\n",
    "                env.set_action_for_agent(behavior_name, agent_id, action)     # send the action to the agent\n",
    "                # env.set_actions(behavior_name, action)                      # send the action to the agent-group\n",
    "        if camera_agent:                                                      # render the screen\n",
    "            decision_steps, _ = env.get_steps(camera_agent)\n",
    "            if (len(decision_steps.agent_id)):\n",
    "                camera_agent_id = decision_steps.agent_id[0]\n",
    "                image = decision_steps[camera_agent_id].obs\n",
    "                if RENDER_REAL_TIME:\n",
    "                    img.set_data(image[0])\n",
    "                    fig.canvas.draw()\n",
    "                else:\n",
    "                    frames.append(image[0])\n",
    "                    \n",
    "\n",
    "        env.step()                                                            # one step through the environment\n",
    "\n",
    "        for behavior_name in agent_info:                                      # for every agent-group in the environment\n",
    "            decision_steps, terminal_steps = env.get_steps(behavior_name)     # get agents' status in the agent-group    \n",
    "            for agent_id in decision_steps.agent_id:\n",
    "                reward = decision_steps[agent_id].reward                      # get the reward\n",
    "                next_state = decision_steps[agent_id].obs                     # get the next state\n",
    "                score[behavior_name][agent_id] += reward\n",
    "                experience[behavior_name][agent_id]['reward'] = reward\n",
    "                experience[behavior_name][agent_id]['next_state'] = next_state\n",
    "            for agent_id in terminal_steps.agent_id:                          # see if episode finished for an agent\n",
    "                reward = terminal_steps[agent_id].reward\n",
    "                score[behavior_name][agent_id] += reward\n",
    "                experience[behavior_name][agent_id]['reward'] = reward\n",
    "                experience[behavior_name][agent_id]['next_state'] = None\n",
    "                terminated[behavior_name][agent_id] = True\n",
    "                if terminal_steps[agent_id].interrupted:\n",
    "                    print('agent #{} in the agent-group \"{}\" has reached the maximum number of steps in the episode'.format(\n",
    "                        agent_id, behavior_name))\n",
    "                    \n",
    "        if camera_agent:                                                      # render the screen\n",
    "            decision_steps, _ = env.get_steps(camera_agent)\n",
    "            if (len(decision_steps.agent_id)):\n",
    "                camera_agent_id = decision_steps.agent_id[0]\n",
    "                image = decision_steps[camera_agent_id].obs\n",
    "                if RENDER_REAL_TIME:\n",
    "                    img.set_data(image[0])\n",
    "                    fig.canvas.draw()\n",
    "                else:\n",
    "                    frames.append(image[0])\n",
    "\n",
    "        # train the RL agent with the experience tuple\n",
    "        # agent.step(experience)\n",
    "\n",
    "        # if EVERY agent in EVERY agent-group is done\n",
    "        if np.asarray([np.asarray(list(agent_group.values())).all() for agent_group in terminated.values()]).all():\n",
    "            break\n",
    "\n",
    "    print(score)\n",
    "\n",
    "if not RENDER_REAL_TIME: animate_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
